---
title: "787 Assignment 1"
author: "Zach Gagnon"
date: 2023-02-15
slug: []
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Install Packages

```{r}
#install.packages("tidyverse")
library(tidyverse)
#install.packages("caret")
library(caret)
library(ggplot2)
```

## Univariate Regression

$$
f(x)=\frac{sin(x)}{x}, \space\space\space x \in [-5\pi, 5\pi]
$$
The sample for the regression will consist of 200 equally spaced values from $-5\pi$ to $5\pi$.

```{r}
n <- 200
x <- seq(from=-5*pi, to=5*pi, length.out=n)
```

The following R code consists of a function that returns f(x) and a vector of random noises from a normal distribution centered at zero with a standard deviation of 0.05.

```{r}
f <- function(x)
{
  return(ifelse(x==0,1,sin(x)/x))
}
sigma <- 0.05
noise <- rnorm(n,0,sigma)
```

Using the above function and noise vector, an n-dimension vector $Y$ is created where each $y_i$ equals $f(x_i)+\varepsilon_i$. An n by 2 matrix $X$ is created consisting of 1s and each $x_i$. Then a data frame of two columns of $X$ and $Y$ is created.

```{r}
y <- c()
for(i in 1:n)
{
  y <- c(y, f(x[i])+noise[i])
}
mat <- matrix(data=c(rep(1,n),x),nrow=n, ncol=2)
df <- data.frame(x=mat[,2], y=y)
```

The data and trace function are plotted below using the traditional plot function and ggplot.

```{r}
plot <- plot(df$x, df$y, xlab="x", ylab="f(x)")
lines(x, f(x))
legend('topleft',"f(x)=sin(x)/x", cex=0.8)

ggplot(df, aes(x, y)) + geom_point() + geom_path(mapping=aes(x=x, y=sin(x)/x))
```

Polynomial regression learning machines are fit to the data, exploring degrees 1 through 6. They are then plotted with smoothing curves.

```{r}
poly1 <- lm(y ~ x, data=df)
poly2 <- lm(y ~ x + I(x^2), data=df)
poly3 <- lm(y ~ x + I(x^2) + I(x^3), data=df)
poly4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=df)
poly5 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data=df)
poly6 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data=df)

ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x)
ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x + I(x^2))
ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x + I(x^2) + I(x^3))
ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x + I(x^2) + I(x^3) + I(x^4))
ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5))
ggplot(df, aes(x, y) ) + geom_point() +
  stat_smooth(method = lm, formula = y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
```

##Predictive Analytics

The following function randomly creates ids used to split the data into a training set and testing set. The split is 2/3 training and 1/3 testing.

```{r}
sho <- function(n, tau)
{
  nte <- round(n*tau)
  ntr <- n-nte
  id.tr <- sample(n)[1:ntr]
  id.te <- setdiff(1:n, id.tr)
  return(list(id.tr=id.tr, id.te=id.te))
}
```

Using this function, the 6 polynomial learners will be fit with the training data. Then we will compute predictions on both the training data and testing data. The training errors and testing errors will be stored in two separate matrices. This process is repeated 10 times.

```{r}
S = 10
set.seed(787369)
mat.te <- matrix(NA, nrow=S, ncol=6)
mat.tr <- matrix(NA, nrow=S, ncol=6)

for(i in 1:S)
{
  predict.ids <- sho(200, 1/3)
  train <- df[predict.ids$id.tr,]
  test <- df[predict.ids$id.te,]
  for(j in 1:6)
  {
    poly <- lm(y ~ poly(x, j, raw=TRUE), data=train)
    predict(poly, train)
    mat.tr[i,j] <- mean((train$y - predict(poly, train))^2)
    predict(poly, test)
    mat.te[i,j] <- mean((train$y - predict(poly, test))^2)
  }
}
```

The following box plots are of the training errors and testing errors, grouped by the polynomial degree of the learner. This is done with both traditional plotting and ggplot.

```{r}
box.tr <- boxplot(mat.tr, ylab="Training Errors", xlab="Polynomial Degree")
box.te <- boxplot(mat.te, ylab="Testing Errors", xlab="Polynomial Degree")

df.tr <- stack(as.data.frame(mat.tr))
colnames(df.tr) <- c("Training_Errors", "Polynomial_Degree")
ggbox.tr <- ggplot(df.tr, aes(x=factor(Polynomial_Degree), y=Training_Errors)) + geom_boxplot()
ggbox.tr
df.te <- stack(as.data.frame(mat.te))
colnames(df.te) <- c("Testing_Errors", "Polynomial_Degree")
ggbox.te <- ggplot(df.te, aes(x=factor(Polynomial_Degree), y=Testing_Errors)) + geom_boxplot()
ggbox.te
```

Lastly, ANOVA is performed on the testing errors and Tukey comparisons are plotted.

```{r}
test.err.lm <- lm(Testing_Errors ~ Polynomial_Degree, df.te)
test.err.anova <- aov(test.err.lm)
summary(test.err.anova)

test.tukey <- plot(TukeyHSD(test.err.anova, conf.level=.95), las=2)
test.tukey
```

